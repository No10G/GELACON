import streamlit as st
import pandas as pd
import numpy as np
import joblib 
import json
import os
import time
from datetime import datetime, date

# --- 0. ãƒ•ã‚¡ã‚¤ãƒ«ã¨å®šæ•°ã®è¨­å®š ---
MODEL_FILE = 'gelecon_predictor_model.pkl'
CACHE_FILE = 'latest_weather_cache.json'
GRADIENT_RATE = 0.6

# â˜… è£œæ­£å€¤ã¨ã‚³ãƒ¼ã‚¹å®šç¾©ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´ã•ã›ã‚‹ï¼‰â˜…
# æ¹¯æ²¢(ç¥ç«‹) - 340måŸºæº– / æ°´ä¸Š(ä¸¸æ²¼) - 370måŸºæº–
COURSE_SETTINGS = {
    # ç¥ç«‹ã‚¹ãƒãƒ¼ãƒªã‚¾ãƒ¼ãƒˆ (æ¹¯æ²¢åŸºæº–: 3.96â„ƒè£œæ­£)
    'Kandatsu_900m': {'resort': 'ç¥ç«‹', 'adj': 3.36, 'elev': 900},
    'Kandatsu_700m': {'resort': 'ç¥ç«‹', 'adj': 2.16, 'elev': 700},
    'Kandatsu_500m': {'resort': 'ç¥ç«‹', 'adj': 0.96, 'elev': 500},
    # ä¸¸æ²¼é«˜åŸ (æ°´ä¸ŠåŸºæº–: 9.78â„ƒè£œæ­£)
    'Marunuma_1950m': {'resort': 'ä¸¸æ²¼', 'adj': 9.48, 'elev': 1950},
    'Marunuma_1700m': {'resort': 'ä¸¸æ²¼', 'adj': 7.98, 'elev': 1700},
    'Marunuma_1500m': {'resort': 'ä¸¸æ²¼', 'adj': 6.78, 'elev': 1500},
    'Marunuma_1300m': {'resort': 'ä¸¸æ²¼', 'adj': 5.58, 'elev': 1300},
}
CONDITIONS = {
    0: 'ãƒ‘ã‚¦ãƒ€ãƒ¼ âœ¨', 1: 'ç¥ãƒãƒ¼ãƒ³ ğŸ’', 2: 'ã‚¢ã‚¤ã‚¹ãƒãƒ¼ãƒ³ âš ï¸', 3: 'ã‚´ãƒ­ã‚´ãƒ­/ã‚·ãƒ£ãƒé›ª â˜€ï¸'
}
MODEL_FEATURE_ORDER = [
    'MaxSnowDepth', 'Snowfall', 'AvgWindSpeed', 'Adj_Temp_Min', 
    'Night_Chill_Factor', 'Cumulative_Heat_History', 'Surface_Hardening_Risk', 'Course_Elev'
]

# --- 1. ãƒ¢ãƒ‡ãƒ«ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ­ãƒ¼ãƒ‰ ---
try:
    model = joblib.load(MODEL_FILE)
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    with open(CACHE_FILE, 'r', encoding='utf-8') as f:
        cache_data = json.load(f)
    model_loaded = True
except Exception as e:
    st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ã¨ãƒ‡ãƒ¼ã‚¿åé›†ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚({e})")
    model_loaded = False


# --- 2. ç‰¹å¾´é‡è¨ˆç®—ã¨äºˆæ¸¬å®Ÿè¡Œé–¢æ•° ---
def calculate_and_predict(course_data, course_key, past_history):
    """æ—¥åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å—ã‘å–ã‚Šã€ç‰¹å¾´é‡è¨ˆç®—ã€ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã€çµæœã‚’è¿”ã™"""
    
    settings = COURSE_SETTINGS[course_key]
    adj_val = settings['adj']
    elev_val = settings['elev']
    
    # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    prev_day_max_temp = past_history['PrevDayMaxTemp']
    cumulative_heat_history = past_history['CumulativeHeatHistoryBase']
    max_snow_depth = past_history['MaxSnowDepth']

    predictions_list = []

    for index, day_data in course_data.iterrows():
        
        # A. æ¨™é«˜è£œæ­£
        adj_min = day_data['MinTemp'] - adj_val
        adj_max = day_data['MaxTemp'] - adj_val
        
        # B. Night Chill Factor: (æ€¥å†·åº¦ = å‰æ—¥Max - å½“æ—¥è£œæ­£Min)
        night_chill = prev_day_max_temp - adj_min
        
        # C. ç´¯ç©ç†±å±¥æ­´ã®æ›´æ–°ã¨è¨ˆç®—
        heat_daily = np.maximum(0, adj_max - 0)
        cumulative_heat_history += heat_daily 
        
        # D. é›ªé¢ç¡¬åŒ–ãƒªã‚¹ã‚¯
        hardening_risk = day_data['AvgWindSpeed']**2 * (1.5 if adj_min < 0 else 1.0)
        
        # E. ãƒ¢ãƒ‡ãƒ«å…¥åŠ›DataFrameã®ä½œæˆ (â˜…é †åºå³å®ˆâ˜…)
        X_predict = pd.DataFrame({
            'MaxSnowDepth': [max_snow_depth], 'Snowfall': [day_data['Snowfall']], 
            'AvgWindSpeed': [day_data['AvgWindSpeed']], 'Adj_Temp_Min': [adj_min], 
            'Night_Chill_Factor': [night_chill], 'Cumulative_Heat_History': [cumulative_heat_history],
            'Surface_Hardening_Risk': [hardening_risk], 'Course_Elev': [elev_val] 
        }, columns=MODEL_FEATURE_ORDER)
        
        # F. XGBoostäºˆæ¸¬å®Ÿè¡Œ
        probabilities = model.predict_proba(X_predict)[0]
        prediction_code = np.argmax(probabilities)
        
        # G. ç¿Œæ—¥ã®ãŸã‚ã«çŠ¶æ…‹ã‚’æ›´æ–°
        prev_day_max_temp = day_data['MaxTemp'] # å½“æ—¥ã®MaxTempã‚’ç¿Œæ—¥ã®PrevDayMaxTempã¨ã—ã¦ä½¿ç”¨

        predictions_list.append({
            'Date': day_data['Date'],
            'Condition': CONDITIONS.get(prediction_code),
            'Probabilities': probabilities,
            'Adj_Min_Temp': adj_min
        })

    return predictions_list


# --- 3. Streamlit UI (ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒãƒ³) ---

st.title("â„ï¸ GELECON AIãƒãƒ¼ãƒ³äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("##### è¤‡æ•°ãƒªã‚¾ãƒ¼ãƒˆãƒ»ã‚³ãƒ¼ã‚¹å¯¾å¿œ (æ¨™é«˜è£œæ­£æ¸ˆã¿)")

if model_loaded:
    
    # ãƒªã‚¾ãƒ¼ãƒˆã®é¸æŠ (ã‚µã‚¤ãƒ‰ãƒãƒ¼)
    resort_options = ['ç¥ç«‹ã‚¹ãƒãƒ¼ãƒªã‚¾ãƒ¼ãƒˆ', 'ä¸¸æ²¼é«˜åŸã‚¹ã‚­ãƒ¼å ´']
    selected_resort = st.sidebar.selectbox("ğŸ”ï¸ ãƒªã‚¾ãƒ¼ãƒˆã‚’é¸æŠ", resort_options)
    st.sidebar.markdown("---")

    # A. é¸æŠãƒªã‚¾ãƒ¼ãƒˆã«å±ã™ã‚‹ã‚³ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if selected_resort == 'ç¥ç«‹ã‚¹ãƒãƒ¼ãƒªã‚¾ãƒ¼ãƒˆ':
        target_keys = [k for k in COURSE_SETTINGS.keys() if 'Kandatsu' in k]
        api_data = cache_data['resorts']['Kandatsu']
    else:
        target_keys = [k for k in COURSE_SETTINGS.keys() if 'Marunuma' in k]
        api_data = cache_data['resorts']['Marunuma']
        
    st.header(f"äºˆæ¸¬å¯¾è±¡: {selected_resort}")
    st.markdown(f"###### ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚åˆ»: {cache_data['timestamp']}")
    
    # B. ã‚³ãƒ¼ã‚¹ã”ã¨ã®äºˆæ¸¬çµæœè¡¨ç¤ºãƒ«ãƒ¼ãƒ— (ãƒ¡ã‚¤ãƒ³ç”»é¢)
    
    # APIã‹ã‚‰å–å¾—ã—ãŸæœªæ¥äºˆå ±ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
    forecast_df = pd.DataFrame(api_data['forecast_data'])
    past_history = api_data['history']

    for course_key in target_keys:
        course_elev = COURSE_SETTINGS[course_key]['elev']
        
        # 3. äºˆæ¸¬ã®å®Ÿè¡Œ (æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—)
        predictions = calculate_and_predict(forecast_df.copy(), course_key, past_history)
        
        st.markdown(f"### ğŸ‚ {course_key} ({course_elev}m)")
        st.markdown("---")

        # çµæœã‚’æ—¥åˆ¥ã§è¡¨ç¤º
        for result in predictions:
            
            prob_df = pd.DataFrame({'Condition': list(CONDITIONS.values()), 'Probability': result['Probabilities']})
            prob_df['Probability'] = (prob_df['Probability'] * 100).round(1)

            with st.expander(f"ğŸ—“ï¸ **{result['Date']}** - äºˆæ¸¬: **{result['Condition']}** ({result['Adj_Min_Temp']:.1f}â„ƒ)"):
                
                # äºˆæ¸¬ã®æ ¹æ‹  (ç¢ºç‡)
                st.subheader("äºˆæ¸¬ã®ç¢ºä¿¡åº¦ã¨å†…è¨³")
                
                col_chart, col_data = st.columns([2, 1])
                
                # ã‚°ãƒ©ãƒ•è¡¨ç¤º (ä¾‹: Plotly/Altairã‚’ä½¿ç”¨ã™ã‚‹ã¨Streamlitã§ç¶ºéº—ã«è¡¨ç¤ºã•ã‚Œã‚‹ãŒã€ã“ã“ã§ã¯Pandasã§ä»£ç”¨)
                # ã‚°ãƒ©ãƒ•ã®ä»£ã‚ã‚Šã«ã€ç¢ºç‡ã®é«˜ã„é †ã«è¡¨ç¤º
                top_prob = prob_df.sort_values('Probability', ascending=False).iloc[0]
                col_chart.metric(f"æœ€ã‚‚ç¢ºä¿¡åº¦ãŒé«˜ã„äºˆæ¸¬", f"{top_prob['Condition']}", f"{top_prob['Probability']}%")

                # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸å†…è¨³
                col_data.markdown("###### ç¢ºç‡ã®å†…è¨³")
                for _, row in prob_df.head(4).iterrows():
                    col_data.write(f"- {row['Condition']}: **{row['Probability']:.1f}%**")