import pandas as pd
import numpy as np
import json
from collections import defaultdict
from datetime import datetime, date, timedelta
import os

# --- 1. å®šæ•°ã¨ãƒ•ã‚¡ã‚¤ãƒ«å ---
# å‚¾æ–œç‡ï¼ˆæ°—æ¸©è£œæ­£ã«ä½¿ç”¨ï¼‰
GRADIENT_RATE = 0.6  
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å
PAST_CACHE_FILE = 'past_data.json'   
FUTURE_CACHE_FILE = 'CF_data.json' 
OUTPUT_CACHE_FILE = 'XGBoost_Features_Cache.json'

# ã‚¢ãƒ¡ãƒ€ã‚¹è¦³æ¸¬æ‰€ã®æ¨™é«˜ï¼ˆæ¦‚ç®—ï¼‰
AMEDAS_ELEVATIONS = {'Kandatsu': 340, 'Marunuma': 370}  
# ã‚³ãƒ¼ã‚¹æ¨™é«˜ï¼ˆäºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
COURSE_TARGETS = {
    'Kandatsu': [900, 700, 500],
    'Marunuma': [1950, 1700, 1500, 1300]
}

# éå»ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬æ‰€ã¨æœªæ¥ãƒ‡ãƒ¼ã‚¿ãƒªã‚¾ãƒ¼ãƒˆã®å¯¾å¿œ
PAST_FUTURE_MAPPING = {
    'yuzawa': 'Kandatsu',
    'minakami': 'Marunuma'
}

# XGBoostãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ç‰¹å¾´é‡ã®é †åº (8åˆ—)
MODEL_FEATURE_ORDER = [
    'MaxSnowDepth', 'Snowfall', 'AvgWindSpeed', 'Adj_Temp_Min', 
    'Night_Chill_Factor', 'Cumulative_Heat_History', 'Surface_Hardening_Risk', 'Course_Elev'
]

# --- 2. ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´é‡è¨ˆç®—é–¢æ•° ---
def generate_xgboost_features():
    
    print("å®šæ•°ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    # 1. ãƒ•ãƒ«ãƒ‘ã‚¹ã®è¨ˆç®—ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—ã—ã€ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã™ã‚‹
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        base_dir = os.getcwd() 

    past_full_path = os.path.join(base_dir, PAST_CACHE_FILE)
    future_full_path = os.path.join(base_dir, FUTURE_CACHE_FILE)

    try:
        # éå»ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        print(f"éå»ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã—ã¦ã„ã¾ã™: {past_full_path}")
        with open(past_full_path, 'r', encoding='utf-8') as f:
            past_cache = json.load(f)
            
        # æœªæ¥ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        print(f"æœªæ¥ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã—ã¦ã„ã¾ã™: {future_full_path}")
        with open(future_full_path, 'r', encoding='utf-8') as f:
            future_cache = json.load(f)
            
        print("âœ… éå»ãƒ‡ãƒ¼ã‚¿ã¨æœªæ¥ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    
    except FileNotFoundError as e:
        print("\n" + "="*50)
        print("ğŸš¨ è‡´å‘½çš„ãªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (FileNotFoundError) ğŸš¨")
        print(f"ã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦ã¿ãŸãƒ•ã‚¡ã‚¤ãƒ«: {e.filename}")
        print("ãƒ•ã‚¡ã‚¤ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("="*50)
        return
    except json.JSONDecodeError:
        print("\n" + "="*50)
        print("ğŸš¨ è‡´å‘½çš„ãªJSONè§£æã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (JSONDecodeError) ğŸš¨")
        print(f"ãƒ•ã‚¡ã‚¤ãƒ« {PAST_CACHE_FILE} ã¾ãŸã¯ {FUTURE_CACHE_FILE} ã®å†…å®¹ãŒä¸æ­£ã§ã™ã€‚")
        print("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹æ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("="*50)
        return

    # éå»ãƒ‡ãƒ¼ã‚¿ã§ä½¿ç”¨ã•ã‚ŒãŸãƒªã‚¾ãƒ¼ãƒˆã‚­ãƒ¼ã‚’æŠ½å‡º
    past_resort_keys = [k for k in past_cache.keys() if k != 'metadata']
    
    # 2. éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸå€¤ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ã‚’è¨ˆç®— (Night Chill Factorã¨ç†±å±¥æ­´ã®åŸºç‚¹)
    initial_history = {}
    for past_key in past_resort_keys:
        # éå»ãƒ‡ãƒ¼ã‚¿ (Næ—¥åˆ†) ã‚’Pandasã«å¤‰æ›
        df_past = pd.DataFrame(past_cache[past_key])
        
        # æ•°å€¤å¤‰æ› (ã‚¨ãƒ©ãƒ¼ã‚’è€ƒæ…®ã—ã€æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ã«å¤‰æ›)
        for col in ['temp_max_c', 'temp_min_c', 'snow_depth_max_cm', 'wind_max_ms']:
            if col in df_past.columns:
                 df_past[col] = pd.to_numeric(df_past[col], errors='coerce')
        
        # éå»ãƒ‡ãƒ¼ã‚¿ã®ä¸è¶³ãƒã‚§ãƒƒã‚¯
        if len(df_past) < 2:
            print(f"ã‚¨ãƒ©ãƒ¼: {past_key} ã®éå»ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ2æ—¥æœªæº€ï¼‰ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
            
        # âš ï¸ å‹•çš„ãªæ—¥ä»˜åŸºç‚¹: æœ€çµ‚æ—¥ã¨ãã®å‰æ—¥ã‚’å‹•çš„ã«ç‰¹å®š âš ï¸
        prev_day_max_temp = df_past['temp_max_c'].iloc[-2] # æœ€æ–°æ—¥ã®å‰æ—¥ã®æœ€é«˜æ°—æ¸©
        df_latest_day = df_past.iloc[-1] # æœ€æ–°æ—¥ã®ãƒ‡ãƒ¼ã‚¿
        
        # è£œæ­£å€¤ã®è¨ˆç®— (éå»ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚ã®è£œæ­£å€¤ã‚’ä»®å®š)
        future_key = PAST_FUTURE_MAPPING[past_key]
        amedas_elev = AMEDAS_ELEVATIONS[future_key]
        course_elev_top = COURSE_TARGETS[future_key][0] # ãƒˆãƒƒãƒ—ã‚³ãƒ¼ã‚¹ã®æ¨™é«˜ã‚’åŸºæº–ã«
        elev_diff = course_elev_top - amedas_elev
        adj_val = (elev_diff / 100) * GRADIENT_RATE 
        
        # ç´¯ç©ç†±å±¥æ­´ã®å†è¨ˆç®— (éå»ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã§å®Ÿè¡Œ)
        df_past['Adj_Max'] = df_past['temp_max_c'] - adj_val
        df_past['Heat_Penalty_Daily'] = np.maximum(0, df_past['Adj_Max'] - 0)
        df_past['CumHeatHistory'] = df_past['Heat_Penalty_Daily'].cumsum()
        
        # æœ€æ–°æ—¥ (Futureã®äºˆæ¸¬é–‹å§‹æ—¥ã®å‰æ—¥) ã®å€¤ã‚’å–å¾—
        cum_heat_base = df_past['CumHeatHistory'].iloc[-1]
        max_snow_depth = df_latest_day['snow_depth_max_cm'] 

        initial_history[past_key] = {
            # è£œæ­£å¾Œã®æœ€é«˜æ°—æ¸©ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã™ã‚‹
            'PrevDayMaxTemp': prev_day_max_temp - adj_val, 
            'CumulativeHeatHistoryBase': cum_heat_base,
            'MaxSnowDepth': max_snow_depth,
            'BaseAdjVal': adj_val
        }

    # 3. æœªæ¥äºˆå ±ãƒ‡ãƒ¼ã‚¿ (CF_data.json) ã‚’æº–å‚™
    all_features_for_model = {}
    
    for base_resort in ['Kandatsu', 'Marunuma']:
        
        # äºˆå ±ãƒ‡ãƒ¼ã‚¿ã®DataFrameã‚’ä½œæˆ
        forecast_data = future_cache.get(base_resort) 
        if not forecast_data:
             print(f"æ³¨æ„: {base_resort} ã®äºˆå ±ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
             continue
             
        forecast_df = pd.DataFrame(forecast_data)
        
        # ğŸš¨ ã‚­ãƒ¼çµ±ä¸€å¾Œã®æ•°å€¤å¤‰æ› ğŸš¨
        # CF_data.jsonã®ã‚­ãƒ¼ã‚’ä½¿ç”¨
        numeric_cols = ['temp_max_c', 'temp_min_c', 'wind_avg_ms', 'snowfall_cm']
        for col in numeric_cols:
            forecast_df[col] = pd.to_numeric(forecast_df[col], errors='coerce') 

        # 4. ã‚³ãƒ¼ã‚¹ã”ã¨ã®ç‰¹å¾´é‡è¨ˆç®—ãƒ«ãƒ¼ãƒ—
        for course_elev in COURSE_TARGETS[base_resort]:
            
            # çŠ¶æ…‹å¤‰æ•°ã®åˆæœŸåŒ– (éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¼•ãç¶™ã)
            past_key = 'yuzawa' if base_resort == 'Kandatsu' else 'minakami'
            if past_key not in initial_history:
                continue 
                
            past_base = initial_history[past_key]
            
            prev_day_max_adj = past_base['PrevDayMaxTemp'] # è£œæ­£æ¸ˆã¿ã®æœ€é«˜æ°—æ¸©
            cum_heat = past_base['CumulativeHeatHistoryBase']
            max_snow_depth = past_base['MaxSnowDepth']

            # è£œæ­£å€¤ã®è¨ˆç®— (ã‚³ãƒ¼ã‚¹ã”ã¨ã«ç•°ãªã‚‹è£œæ­£ã‚’é©ç”¨)
            amedas_elev = AMEDAS_ELEVATIONS[base_resort]
            elev_diff = course_elev - amedas_elev
            adjustment_value = (elev_diff / 100) * GRADIENT_RATE
            
            course_features = []
            
            for index, day_data in forecast_df.iterrows():
                
                # A. æ¨™é«˜è£œæ­£ (çµ±ä¸€ã‚­ãƒ¼ã‚’ä½¿ç”¨)
                adj_min = day_data['temp_min_c'] - adjustment_value
                adj_max = day_data['temp_max_c'] - adjustment_value
                
                # B. Night Chill Factor (æ€¥å†·åº¦)
                night_chill = prev_day_max_adj - adj_min
                
                # C. ç´¯ç©ç†±å±¥æ­´ã®æ›´æ–°
                heat_daily = np.maximum(0, adj_max - 0)
                cum_heat = cum_heat + heat_daily 
                
                # D. é›ªé¢ç¡¬åŒ–ãƒªã‚¹ã‚¯ (çµ±ä¸€ã‚­ãƒ¼ã‚’ä½¿ç”¨)
                hardening_risk = day_data['wind_avg_ms']**2 * (1.5 if adj_min < 0 else 1.0)
                
                # E. XGBoostãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ (é †åºå³å®ˆ)
                record = {
                    'Date': day_data['date'], 
                    'MaxSnowDepth': max_snow_depth, 
                    'Snowfall': day_data['snowfall_cm'], # çµ±ä¸€ã‚­ãƒ¼ã‚’ä½¿ç”¨
                    'AvgWindSpeed': day_data['wind_avg_ms'], # çµ±ä¸€ã‚­ãƒ¼ã‚’ä½¿ç”¨
                    'Adj_Temp_Min': adj_min, 
                    'Night_Chill_Factor': night_chill, 
                    'Cumulative_Heat_History': cum_heat,
                    'Surface_Hardening_Risk': hardening_risk, 
                    'Course_Elev': course_elev
}
                
                # ãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’ã€æœŸå¾…ã™ã‚‹é †åºã§æ ¼ç´
                feature_values = [
                    float(record[feat]) if isinstance(record[feat], (int, float, np.number)) else record[feat] 
                    for feat in MODEL_FEATURE_ORDER
                ]
                # æ—¥ä»˜æƒ…å ±ã¨ç‰¹å¾´é‡ã‚’æ ¼ç´
                final_record = {'Date': record['Date'], 'Course': course_elev, 'Features': feature_values}
                course_features.append(final_record)
                
                # F. ç¿Œæ—¥ã®ãŸã‚ã«çŠ¶æ…‹ã‚’æ›´æ–°
                prev_day_max_adj = adj_max # è£œæ­£å¾Œã®æœ€é«˜æ°—æ¸©ã‚’ç¿Œæ—¥ã®åŸºç‚¹ã«ã™ã‚‹

            all_features_for_model[f"{base_resort}_{course_elev}m"] = course_features
            
    # 5. æœ€çµ‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®å‡ºåŠ›
    output_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "features": all_features_for_model
    }
    output_filename_full_path = os.path.join(base_dir, OUTPUT_CACHE_FILE)
    
    with open(output_filename_full_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)

    print(f"\nâœ… ç‰¹å¾´é‡è¨ˆç®—ãŒå®Œäº†ã—ã€XGBoostäºˆæ¸¬ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ '{output_filename_full_path}' ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")


# --- å®Ÿè¡Œ ---
if __name__ == '__main__':
    generate_xgboost_features()